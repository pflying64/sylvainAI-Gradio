API REFERENCE
1) Introduction

Welcome to the ElevenLabs API reference.

2) Installation
You can interact with the API through HTTP or Websocket requests from any language, via our official Python bindings or our official Node.js libraries.

To install the official Python bindings, run the following command:

pip install elevenlabs

To install the official Node.js library, run the following command in your Node.js project directory:

npm install elevenlabs


3) Authentication

API Keys
The ElevenLabs API uses API keys for authentication. Every request to the API must include your API key, used to authenticate your requests and track usage quota.

Each API key can be scoped to one of the following:

Scope restriction: Set access restrictions by limiting which API endpoints the key can access.
Credit quota: Define custom credit limits to control usage.
Remember that your API key is a secret. Do not share it with others or expose it in any client-side code (browsers, apps).

All API requests should include your API key in an xi-api-key HTTP header as follows:

xi-api-key: ELEVENLABS_API_KEY

4) Making requests
You can paste the command below into your terminal to run your first API request. Make sure to replace $ELEVENLABS_API_KEY with your secret API key.

curl 'https://api.elevenlabs.io/v1/models' \
  -H 'Content-Type: application/json' \
  -H 'xi-api-key: $ELEVENLABS_API_KEY'

Example with the elevenlabs Python package:

from elevenlabs.client import ElevenLabs
client = ElevenLabs(
  api_key='YOUR_API_KEY',
)

Example with the elevenlabs Node.js package:

import { ElevenLabsClient } from 'elevenlabs';
const client = new ElevenLabsClient({
  apiKey: 'YOUR_API_KEY',
});

5) Streaming

The ElevenLabs API supports real-time audio streaming for select endpoints, returning raw audio bytes (e.g., MP3 data) directly over HTTP using chunked transfer encoding. This allows clients to process or play audio incrementally as it is generated.

Our official Node and Python libraries include utilities to simplify handling this continuous audio stream.

Streaming is supported for the Text to Speech API, Voice Changer API & Audio Isolation API. This section focuses on how streaming works for requests made to the Text to Speech API.

In Python, a streaming request looks like:

from elevenlabs import stream
from elevenlabs.client import ElevenLabs
client = ElevenLabs()
audio_stream = client.text_to_speech.convert_as_stream(
    text="This is a test",
    voice_id="JBFqnCBsd6RMkjVDRZzb",
    model_id="eleven_multilingual_v2"
)
# option 1: play the streamed audio locally
stream(audio_stream)
# option 2: process the audio bytes manually
for chunk in audio_stream:
    if isinstance(chunk, bytes):
        print(chunk)

In Node / Typescript, a streaming request looks like:

import { ElevenLabsClient, stream } from 'elevenlabs';
import { Readable } from 'stream';
const client = new ElevenLabsClient();
async function main() {
  const audioStream = await client.textToSpeech.convertAsStream('JBFqnCBsd6RMkjVDRZzb', {
    text: 'This is a test',
    model_id: 'eleven_multilingual_v2',
  });
  // option 1: play the streamed audio locally
  await stream(Readable.from(audioStream));
  // option 2: process the audio manually
  for await (const chunk of audioStream) {
    console.log(chunk);
  }
}
main();

6) WebSockets

GET


wss://api.elevenlabs.io
/v1/text-to-speech/:voice_id/stream-input
Handshake
URL	wss://api.elevenlabs.io/v1/text-to-speech/{voice_id}/stream-input
Method	GET
Status	101 Switching Protocols
Try it
Messages

{"text":" ","voice_settings":{"stability":0.5,"similarity_boost":0.8,"speed":1}}
publish


{"text":"Hello World","try_trigger_generation":true}
publish


{"text":""}
publish


{"audio":"Y3VyaW91cyBtaW5kcyB0aGluayBhbGlrZSA6KQ==","normalizedAlignment":{"char_start_times_ms":[0,3,7,9,11,12,13,15,17,19,21],"chars_durations_ms":[3,4,2,2,1,1,2,2,2,2,3],"chars":["H","e","l","l","o"," ","w","o","r","l","d"]},"alignment":{"char_start_times_ms":[0,3,7,9,11,12,13,15,17,19,21],"chars_durations_ms":[3,4,2,2,1,1,2,2,2,2,3],"chars":["H","e","l","l","o"," ","w","o","r","l","d"]}}
subscribe

The Text-to-Speech WebSockets API is designed to generate audio from partial text input while ensuring consistency throughout the generated audio. Although highly flexible, the WebSockets API isn’t a one-size-fits-all solution. It’s well-suited for scenarios where:

The input text is being streamed or generated in chunks.
Word-to-audio alignment information is required.
However, it may not be the best choice when:

The entire input text is available upfront. Given that the generations are partial, some buffering is involved, which could potentially result in slightly higher latency compared to a standard HTTP request.
You want to quickly experiment or prototype. Working with WebSockets can be harder and more complex than using a standard HTTP API, which might slow down rapid development and testing.
Handshake
GET


wss://api.elevenlabs.io
/v1/text-to-speech/:voice_id/stream-input

Path parameters
voice_id
string
Required
The unique identifier for the voice to use in the TTS process.

Query parameters
model_id
string
Optional
The model ID to use

language_code
string
Optional
The ISO 639-1 language code (for Turbo v2.5 and Flash v2.5 models only)

enable_logging
string
Optional
Whether to enable logging of the request

enable_ssml_parsing
boolean
Optional
Defaults to false
Whether to enable SSML parsing

optimize_streaming_latency
enum
Optional
Defaults to 0
Deprecated
Latency optimization level (deprecated)

Allowed values:
0
1
2
3
4
output_format
enum
Optional
Defaults to mp3_44100
The output audio format


Search...

mp3_44100
pcm_16000
pcm_22050
pcm_24000
pcm_44100
ulaw_8000
inactivity_timeout
double
Optional
Defaults to 20
Timeout for inactivity before connection is closed

sync_alignment
boolean
Optional
Defaults to false
Whether to include timing data with every audio chunk

auto_mode
boolean
Optional
Defaults to false
This parameter focuses on reducing the latency by disabling the chunk schedule and all buffers. It is only recommended when sending full sentences or phrases, sending partial phrases will result in highly reduced quality. By default it’s set to false.

apply_text_normalization
enum
Optional
Defaults to auto
This parameter controls text normalization with three modes - ‘auto’, ‘on’, and ‘off’. When set to ‘auto’, the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With ‘on’, text normalization will always be applied, while with ‘off’, it will be skipped. Cannot be turned on for ‘eleven_turbo_v2_5’ model. Defaults to ‘auto’.

Allowed values:
auto
on
off
seed
integer
Optional
>=0
If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. Must be an integer between 0 and 4294967295.

Send
Initialize Connection
object
Required

Hide 6 properties
text
" "
Required
The initial text that must be sent is a blank space.

voice_settings
object
Optional

Show 5 properties
generation_config
object
Optional

Show 1 properties
pronunciation_dictionary_locators
list of objects
Optional
Optional list of pronunciation dictionary locators. If provided, these dictionaries will be used to modify pronunciation of matching text. Must only be provided in the first message.

Note: Pronunciation dictionary matches will only be respected within a provided chunk.


Show 2 properties
xi-api-key
string
Optional
Your ElevenLabs API key. This can only be included in the first message and is not needed if present in the header.

authorization
string
Optional
Your authorization bearer token. This can only be included in the first message and is not needed if present in the header.

OR
Send Text
object
Required

Hide 5 properties
text
string
Required
The text to be sent to the API for audio generation. Should always end with a single space string.

try_trigger_generation
boolean
Optional
Defaults to false
This is an advanced setting that most users shouldn’t need to use. It relates to our generation schedule.

Use this to attempt to immediately trigger the generation of audio, overriding the chunk_length_schedule. Unlike flush, try_trigger_generation will only generate audio if our buffer contains more than a minimum threshold of characters, this is to ensure a higher quality response from our model.

Note that overriding the chunk schedule to generate small amounts of text may result in lower quality audio, therefore, only use this parameter if you really need text to be processed immediately. We generally recommend keeping the default value of false and adjusting the chunk_length_schedule in the generation_config instead.

voice_settings
object
Optional
The voice settings field can be provided in the first InitializeConnection message and then must either be not provided or not changed.


Show 5 properties
generator_config
object
Optional
The generator config field can be provided in the first InitializeConnection message and then must either be not provided or not changed.


Show 1 properties
flush
boolean
Optional
Defaults to false
Flush forces the generation of audio. Set this value to true when you have finished sending text, but want to keep the websocket connection open.

This is useful when you want to ensure that the last chunk of audio is generated even when the length of text sent is smaller than the value set in chunk_length_schedule (e.g. 120 or 50).

OR
Close Connection
object
Required

Hide 1 properties
text
""
Required
End the stream with an empty string

Receive
Audio Output
object
Required

Hide 3 properties
audio
string
Required
A generated partial audio chunk, encoded using the selected output_format, by default this is MP3 encoded as a base64 string.

normalizedAlignment
object
Optional
Alignment information for the generated audio given the input normalized text sequence.

properties

char_start_times_ms
list of integers
Optional
A list of starting times (in milliseconds) for each character in the normalized text as it corresponds to the audio. For instance, the character ‘H’ starts at time 0 ms in the audio. Note these times are relative to the returned chunk from the model, and not the full audio response.

chars_durations_ms
list of integers
Optional
A list of durations (in milliseconds) for each character in the normalized text as it corresponds to the audio. For instance, the character ‘H’ lasts for 3 ms in the audio. Note these times are relative to the returned chunk from the model, and not the full audio response.

chars
list of strings
Optional
A list of characters in the normalized text sequence. For instance, the first character is ‘H’. Note that this list may contain spaces, punctuation, and other special characters. The length of this list should be the same as the lengths of char_start_times_ms and chars_durations_ms.

alignment
object
Optional
Alignment information for the generated audio given the input text sequence.

properties
char_start_times_ms
list of integers
Optional
A list of starting times (in milliseconds) for each character in the text as it corresponds to the audio. For instance, the character ‘H’ starts at time 0 ms in the audio. Note these times are relative to the returned chunk from the model, and not the full audio response.

chars_durations_ms
list of integers
Optional
A list of durations (in milliseconds) for each character in the text as it corresponds to the audio. For instance, the character ‘H’ lasts for 3 ms in the audio. Note these times are relative to the returned chunk from the model, and not the full audio response.

chars
list of strings
Optional
A list of characters in the text sequence. For instance, the first character is ‘H’. Note that this list may contain spaces, punctuation, and other special characters. The length of this list should be the same as the lengths of char_start_times_ms and chars_durations_ms.

OR
Final Output
object
Required

Hide 1 properties
isFinal
true
Optional
Defaults to true
Indicates if the generation is complete. If set to True, audio will be null.


7) Create speech

POST


https://api.elevenlabs.io
/v1/text-to-speech/:voice_id
POST

/v1/text-to-speech/:voice_id

cURL

curl -X POST "https://api.elevenlabs.io/v1/text-to-speech/JBFqnCBsd6RMkjVDRZzb?output_format=mp3_44100_128" \
     -H "xi-api-key: <apiKey>" \
     -H "Content-Type: application/json" \
     -d '{
  "text": "The first move is what sets everything in motion.",
  "model_id": "eleven_multilingual_v2"
}'
Try it
Converts text into speech using a voice of your choice and returns audio.

Path parameters
voice_id
string
Required
ID of the voice to be used. Use the Get voices endpoint list all the available voices.

Headers
xi-api-key
string
Required
Query parameters
enable_logging
boolean
Optional
Defaults to true
When enable_logging is set to false zero retention mode will be used for the request. This will mean history features are unavailable for this request, including request stitching. Zero retention mode may only be used by enterprise customers.

optimize_streaming_latency
integer
Optional
Deprecated
You can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values: 0 - default mode (no latency optimizations) 1 - normal latency optimizations (about 50% of possible latency improvement of option 3) 2 - strong latency optimizations (about 75% of possible latency improvement of option 3) 3 - max latency optimizations 4 - max latency optimizations, but also with text normalizer turned off for even more latency savings (best latency, but can mispronounce eg numbers and dates).

Defaults to None.

output_format
enum
Optional
Defaults to mp3_44100_128
Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the μ-law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs.


Show 12 enum values
Request
This endpoint expects an object.
text
string
Required
The text that will get converted into speech.

model_id
string
Optional
Defaults to eleven_monolingual_v1
Identifier of the model that will be used, you can query them using GET /v1/models. The model needs to have support for text to speech, you can check this using the can_do_text_to_speech property.

language_code
string
Optional
Language code (ISO 639-1) used to enforce a language for the model. Currently only Turbo v2.5 and Flash v2.5 support language enforcement. For other models, an error will be returned if language code is provided.

voice_settings
object
Optional
Voice settings overriding stored settings for the given voice. They are applied only on the given request.


Show 5 properties
pronunciation_dictionary_locators
list of objects
Optional
A list of pronunciation dictionary locators (id, version_id) to be applied to the text. They will be applied in order. You may have up to 3 locators per request


Show 2 properties
seed
integer
Optional
If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. Must be integer between 0 and 4294967295.

previous_text
string
Optional
The text that came before the text of the current request. Can be used to improve the speech’s continuity when concatenating together multiple generations or to influence the speech’s continuity in the current generation.

next_text
string
Optional
The text that comes after the text of the current request. Can be used to improve the speech’s continuity when concatenating together multiple generations or to influence the speech’s continuity in the current generation.

previous_request_ids
list of strings
Optional
A list of request_id of the samples that were generated before this generation. Can be used to improve the speech’s continuity when splitting up a large task into multiple requests. The results will be best when the same model is used across the generations. In case both previous_text and previous_request_ids is send, previous_text will be ignored. A maximum of 3 request_ids can be send.

next_request_ids
list of strings
Optional
A list of request_id of the samples that come after this generation. next_request_ids is especially useful for maintaining the speech’s continuity when regenerating a sample that has had some audio quality issues. For example, if you have generated 3 speech clips, and you want to improve clip 2, passing the request id of clip 3 as a next_request_id (and that of clip 1 as a previous_request_id) will help maintain natural flow in the combined speech. The results will be best when the same model is used across the generations. In case both next_text and next_request_ids is send, next_text will be ignored. A maximum of 3 request_ids can be send.

apply_text_normalization
enum
Optional
Defaults to auto
This parameter controls text normalization with three modes: ‘auto’, ‘on’, and ‘off’. When set to ‘auto’, the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With ‘on’, text normalization will always be applied, while with ‘off’, it will be skipped. Cannot be turned on for ‘eleven_turbo_v2_5’ model.

Allowed values:
auto
on
off
use_pvc_as_ivc
boolean
Optional
Defaults to false
Deprecated
If true, we won’t use PVC version of the voice for the generation but the IVC version. This is a temporary workaround for higher latency in PVC versions.

Response
The generated audio file

Errors

422
Text to Speech Convert Request Unprocessable Entity Error


Create speech with timing

POST


https://api.elevenlabs.io
/v1/text-to-speech/:voice_id/with-timestamps
POST

/v1/text-to-speech/:voice_id/with-timestamps

cURL

curl -X POST https://api.elevenlabs.io/v1/text-to-speech/21m00Tcm4TlvDq8ikWAM/with-timestamps \
     -H "xi-api-key: <apiKey>" \
     -H "Content-Type: application/json" \
     -d '{
  "text": "This is a test for the API of ElevenLabs."
}'
Try it
200
Successful

{
  "audio_base64": "base64_encoded_audio_string",
  "alignment": {
    "characters": [
      "H",
      "e",
      "l",
      "l",
      "o"
    ],
    "character_start_times_seconds": [
      0,
      0.1,
      0.2,
      0.3,
      0.4
    ],
    "character_end_times_seconds": [
      0.1,
      0.2,
      0.3,
      0.4,
      0.5
    ]
  },
  "normalized_alignment": {
    "characters": [
      "H",
      "e",
      "l",
      "l",
      "o"
    ],
    "character_start_times_seconds": [
      0,
      0.1,
      0.2,
      0.3,
      0.4
    ],
    "character_end_times_seconds": [
      0.1,
      0.2,
      0.3,
      0.4,
      0.5
    ]
  }
}
Generate speech from text with precise character-level timing information for audio-text synchronization.

Path parameters
voice_id
string
Required
Voice ID to be used, you can use https://api.elevenlabs.io/v1/voices to list all the available voices.

Headers
xi-api-key
string
Required
Query parameters
enable_logging
boolean
Optional
Defaults to true
When enable_logging is set to false zero retention mode will be used for the request. This will mean history features are unavailable for this request, including request stitching. Zero retention mode may only be used by enterprise customers.

optimize_streaming_latency
integer
Optional
Deprecated
You can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values: 0 - default mode (no latency optimizations) 1 - normal latency optimizations (about 50% of possible latency improvement of option 3) 2 - strong latency optimizations (about 75% of possible latency improvement of option 3) 3 - max latency optimizations 4 - max latency optimizations, but also with text normalizer turned off for even more latency savings (best latency, but can mispronounce eg numbers and dates).

Defaults to None.

output_format
enum
Optional
Defaults to mp3_44100_128
Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the μ-law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs.


Show 12 enum values
Request
This endpoint expects an object.
text
string
Required
The text that will get converted into speech.

model_id
string
Optional
Defaults to eleven_monolingual_v1
Identifier of the model that will be used, you can query them using GET /v1/models. The model needs to have support for text to speech, you can check this using the can_do_text_to_speech property.

language_code
string
Optional
Language code (ISO 639-1) used to enforce a language for the model. Currently only Turbo v2.5 and Flash v2.5 support language enforcement. For other models, an error will be returned if language code is provided.

voice_settings
object
Optional
Voice settings overriding stored settings for the given voice. They are applied only on the given request.


Show 5 properties
pronunciation_dictionary_locators
list of objects
Optional
A list of pronunciation dictionary locators (id, version_id) to be applied to the text. They will be applied in order. You may have up to 3 locators per request


Show 2 properties
seed
integer
Optional
If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. Must be integer between 0 and 4294967295.

previous_text
string
Optional
The text that came before the text of the current request. Can be used to improve the speech’s continuity when concatenating together multiple generations or to influence the speech’s continuity in the current generation.

next_text
string
Optional
The text that comes after the text of the current request. Can be used to improve the speech’s continuity when concatenating together multiple generations or to influence the speech’s continuity in the current generation.

previous_request_ids
list of strings
Optional
A list of request_id of the samples that were generated before this generation. Can be used to improve the speech’s continuity when splitting up a large task into multiple requests. The results will be best when the same model is used across the generations. In case both previous_text and previous_request_ids is send, previous_text will be ignored. A maximum of 3 request_ids can be send.

next_request_ids
list of strings
Optional
A list of request_id of the samples that come after this generation. next_request_ids is especially useful for maintaining the speech’s continuity when regenerating a sample that has had some audio quality issues. For example, if you have generated 3 speech clips, and you want to improve clip 2, passing the request id of clip 3 as a next_request_id (and that of clip 1 as a previous_request_id) will help maintain natural flow in the combined speech. The results will be best when the same model is used across the generations. In case both next_text and next_request_ids is send, next_text will be ignored. A maximum of 3 request_ids can be send.

apply_text_normalization
enum
Optional
Defaults to auto
This parameter controls text normalization with three modes: ‘auto’, ‘on’, and ‘off’. When set to ‘auto’, the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With ‘on’, text normalization will always be applied, while with ‘off’, it will be skipped. Cannot be turned on for ‘eleven_turbo_v2_5’ model.

Allowed values:
auto
on
off
use_pvc_as_ivc
boolean
Optional
Defaults to false
Deprecated
If true, we won’t use PVC version of the voice for the generation but the IVC version. This is a temporary workaround for higher latency in PVC versions.

Response
Successful Response

audio_base64
string
Base64 encoded audio data

alignment
object
Optional
Timestamp information for each character in the original text


Show 3 properties
normalized_alignment
object
Optional
Timestamp information for each character in the normalized text


Show 3 properties
Errors

422
Text to Speech Convert with Timestamps Request Unprocessable Entity Error

Stream speech

POST


https://api.elevenlabs.io
/v1/text-to-speech/:voice_id/stream
POST

/v1/text-to-speech/:voice_id/stream

cURL

curl -X POST "https://api.elevenlabs.io/v1/text-to-speech/JBFqnCBsd6RMkjVDRZzb/stream?output_format=mp3_44100_128" \
     -H "xi-api-key: <apiKey>" \
     -H "Content-Type: application/json" \
     -d '{
  "text": "The first move is what sets everything in motion.",
  "model_id": "eleven_multilingual_v2"
}'
Try it
Converts text into speech using a voice of your choice and returns audio as an audio stream.

Path parameters
voice_id
string
Required
ID of the voice to be used. Use the Get voices endpoint list all the available voices.

Headers
xi-api-key
string
Required
Query parameters
enable_logging
boolean
Optional
Defaults to true
When enable_logging is set to false zero retention mode will be used for the request. This will mean history features are unavailable for this request, including request stitching. Zero retention mode may only be used by enterprise customers.

optimize_streaming_latency
integer
Optional
Deprecated
You can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values: 0 - default mode (no latency optimizations) 1 - normal latency optimizations (about 50% of possible latency improvement of option 3) 2 - strong latency optimizations (about 75% of possible latency improvement of option 3) 3 - max latency optimizations 4 - max latency optimizations, but also with text normalizer turned off for even more latency savings (best latency, but can mispronounce eg numbers and dates).

Defaults to None.

output_format
enum
Optional
Defaults to mp3_44100_128
Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the μ-law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs.


Show 12 enum values
Request
This endpoint expects an object.
text
string
Required
The text that will get converted into speech.

model_id
string
Optional
Defaults to eleven_monolingual_v1
Identifier of the model that will be used, you can query them using GET /v1/models. The model needs to have support for text to speech, you can check this using the can_do_text_to_speech property.

language_code
string
Optional
Language code (ISO 639-1) used to enforce a language for the model. Currently only Turbo v2.5 and Flash v2.5 support language enforcement. For other models, an error will be returned if language code is provided.

voice_settings
object
Optional
Voice settings overriding stored settings for the given voice. They are applied only on the given request.


Show 5 properties
pronunciation_dictionary_locators
list of objects
Optional
A list of pronunciation dictionary locators (id, version_id) to be applied to the text. They will be applied in order. You may have up to 3 locators per request


Show 2 properties
seed
integer
Optional
If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. Must be integer between 0 and 4294967295.

previous_text
string
Optional
The text that came before the text of the current request. Can be used to improve the speech’s continuity when concatenating together multiple generations or to influence the speech’s continuity in the current generation.

next_text
string
Optional
The text that comes after the text of the current request. Can be used to improve the speech’s continuity when concatenating together multiple generations or to influence the speech’s continuity in the current generation.

previous_request_ids
list of strings
Optional
A list of request_id of the samples that were generated before this generation. Can be used to improve the speech’s continuity when splitting up a large task into multiple requests. The results will be best when the same model is used across the generations. In case both previous_text and previous_request_ids is send, previous_text will be ignored. A maximum of 3 request_ids can be send.

next_request_ids
list of strings
Optional
A list of request_id of the samples that come after this generation. next_request_ids is especially useful for maintaining the speech’s continuity when regenerating a sample that has had some audio quality issues. For example, if you have generated 3 speech clips, and you want to improve clip 2, passing the request id of clip 3 as a next_request_id (and that of clip 1 as a previous_request_id) will help maintain natural flow in the combined speech. The results will be best when the same model is used across the generations. In case both next_text and next_request_ids is send, next_text will be ignored. A maximum of 3 request_ids can be send.

apply_text_normalization
enum
Optional
Defaults to auto
This parameter controls text normalization with three modes: ‘auto’, ‘on’, and ‘off’. When set to ‘auto’, the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With ‘on’, text normalization will always be applied, while with ‘off’, it will be skipped. Cannot be turned on for ‘eleven_turbo_v2_5’ model.

Allowed values:
auto
on
off
use_pvc_as_ivc
boolean
Optional
Defaults to false
Deprecated
If true, we won’t use PVC version of the voice for the generation but the IVC version. This is a temporary workaround for higher latency in PVC versions.

Response
Streaming audio data

Errors

422
Text to Speech Convert as Stream Request Unprocessable Entity Error

Stream speech with timing

POST


https://api.elevenlabs.io
/v1/text-to-speech/:voice_id/stream/with-timestamps
POST

/v1/text-to-speech/:voice_id/stream/with-timestamps

cURL

curl -X POST "https://api.elevenlabs.io/v1/text-to-speech/JBFqnCBsd6RMkjVDRZzb/stream/with-timestamps?output_format=mp3_44100_128" \
     -H "xi-api-key: <apiKey>" \
     -H "Content-Type: application/json" \
     -d '{
  "text": "The first move is what sets everything in motion.",
  "model_id": "eleven_multilingual_v2"
}'
Try it
Converts text into speech using a voice of your choice and returns a stream of JSONs containing audio as a base64 encoded string together with information on when which character was spoken.

Path parameters
voice_id
string
Required
ID of the voice to be used. Use the Get voices endpoint list all the available voices.

Headers
xi-api-key
string
Required
Query parameters
enable_logging
boolean
Optional
Defaults to true
When enable_logging is set to false zero retention mode will be used for the request. This will mean history features are unavailable for this request, including request stitching. Zero retention mode may only be used by enterprise customers.

optimize_streaming_latency
integer
Optional
Deprecated
You can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values: 0 - default mode (no latency optimizations) 1 - normal latency optimizations (about 50% of possible latency improvement of option 3) 2 - strong latency optimizations (about 75% of possible latency improvement of option 3) 3 - max latency optimizations 4 - max latency optimizations, but also with text normalizer turned off for even more latency savings (best latency, but can mispronounce eg numbers and dates).

Defaults to None.

output_format
enum
Optional
Defaults to mp3_44100_128
Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the μ-law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs.


Search...

mp3_22050_32
mp3_44100_32
mp3_44100_64
mp3_44100_96
mp3_44100_128
mp3_44100_192
pcm_8000
pcm_16000
pcm_22050
pcm_24000
pcm_44100
ulaw_8000
Request
This endpoint expects an object.
text
string
Required
The text that will get converted into speech.

model_id
string
Optional
Defaults to eleven_monolingual_v1
Identifier of the model that will be used, you can query them using GET /v1/models. The model needs to have support for text to speech, you can check this using the can_do_text_to_speech property.

language_code
string
Optional
Language code (ISO 639-1) used to enforce a language for the model. Currently only Turbo v2.5 and Flash v2.5 support language enforcement. For other models, an error will be returned if language code is provided.

voice_settings
object
Optional
Voice settings overriding stored settings for the given voice. They are applied only on the given request.


Hide 5 properties
stability
double
Required
Determines how stable the voice is and the randomness between each generation. Lower values introduce broader emotional range for the voice. Higher values can result in a monotonous voice with limited emotion.

similarity_boost
double
Required
Determines how closely the AI should adhere to the original voice when attempting to replicate it.

style
double
Optional
Defaults to 0
Determines the style exaggeration of the voice. This setting attempts to amplify the style of the original speaker. It does consume additional computational resources and might increase latency if set to anything other than 0.

use_speaker_boost
boolean
Optional
Defaults to true
This setting boosts the similarity to the original speaker. Using this setting requires a slightly higher computational load, which in turn increases latency.

speed
double
Optional
Defaults to 1
Controls the speed of the generated speech. Values range from 0.7 to 1.2, with 1.0 being the default speed. Lower values create slower, more deliberate speech while higher values produce faster-paced speech. Extreme values can impact the quality of the generated speech.

pronunciation_dictionary_locators
list of objects
Optional
A list of pronunciation dictionary locators (id, version_id) to be applied to the text. They will be applied in order. You may have up to 3 locators per request


Hide 2 properties
pronunciation_dictionary_id
string
Required
version_id
string
Required
seed
integer
Optional
If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. Must be integer between 0 and 4294967295.

previous_text
string
Optional
The text that came before the text of the current request. Can be used to improve the speech’s continuity when concatenating together multiple generations or to influence the speech’s continuity in the current generation.

next_text
string
Optional
The text that comes after the text of the current request. Can be used to improve the speech’s continuity when concatenating together multiple generations or to influence the speech’s continuity in the current generation.

previous_request_ids
list of strings
Optional
A list of request_id of the samples that were generated before this generation. Can be used to improve the speech’s continuity when splitting up a large task into multiple requests. The results will be best when the same model is used across the generations. In case both previous_text and previous_request_ids is send, previous_text will be ignored. A maximum of 3 request_ids can be send.

next_request_ids
list of strings
Optional
A list of request_id of the samples that come after this generation. next_request_ids is especially useful for maintaining the speech’s continuity when regenerating a sample that has had some audio quality issues. For example, if you have generated 3 speech clips, and you want to improve clip 2, passing the request id of clip 3 as a next_request_id (and that of clip 1 as a previous_request_id) will help maintain natural flow in the combined speech. The results will be best when the same model is used across the generations. In case both next_text and next_request_ids is send, next_text will be ignored. A maximum of 3 request_ids can be send.

apply_text_normalization
enum
Optional
Defaults to auto
This parameter controls text normalization with three modes: ‘auto’, ‘on’, and ‘off’. When set to ‘auto’, the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With ‘on’, text normalization will always be applied, while with ‘off’, it will be skipped. Cannot be turned on for ‘eleven_turbo_v2_5’ model.

Allowed values:
auto
on
off
use_pvc_as_ivc
boolean
Optional
Defaults to false
Deprecated
If true, we won’t use PVC version of the voice for the generation but the IVC version. This is a temporary workaround for higher latency in PVC versions.

Response
Stream of transcription chunks

audio_base64
string
Base64 encoded audio data

alignment
object
Optional
Timestamp information for each character in the original text


Hide 3 properties
characters
list of strings
character_start_times_seconds
list of doubles
character_end_times_seconds
list of doubles
normalized_alignment
object
Optional
Timestamp information for each character in the normalized text


Hide 3 properties
characters
list of strings
character_start_times_seconds
list of doubles
character_end_times_seconds
list of doubles
Errors

422
Text to Speech Stream with Timestamps Request Unprocessable Entity Error


Create transcript

POST


https://api.elevenlabs.io
/v1/speech-to-text
POST

/v1/speech-to-text

cURL

curl -X POST https://api.elevenlabs.io/v1/speech-to-text \
     -H "xi-api-key: <apiKey>" \
     -H "Content-Type: multipart/form-data" \
     -F model_id="model_id" \
     -F file=@<file1>
Try it
200
Successful

{
  "language_code": "en",
  "language_probability": 0.98,
  "text": "Hello world!",
  "words": [
    {
      "text": "Hello",
      "type": "word",
      "start": 0,
      "end": 0.5,
      "speaker_id": "speaker_1",
      "characters": [
        {
          "text": "text",
          "start": 0,
          "end": 0.1
        }
      ]
    },
    {
      "text": " ",
      "type": "spacing",
      "start": 0.5,
      "end": 0.5,
      "speaker_id": "speaker_1",
      "characters": [
        {
          "text": "text",
          "start": 0,
          "end": 0.1
        }
      ]
    },
    {
      "text": "world!",
      "type": "word",
      "start": 0.5,
      "end": 1.2,
      "speaker_id": "speaker_1",
      "characters": [
        {
          "text": "text",
          "start": 0,
          "end": 0.1
        }
      ]
    }
  ]
}
Transcribe an audio or video file.

Headers
xi-api-key
string
Required
Query parameters
enable_logging
boolean
Optional
Defaults to true
When enable_logging is set to false zero retention mode will be used for the request. This will mean history features are unavailable for this request, including request stitching. Zero retention mode may only be used by enterprise customers.

Request
This endpoint expects a multipart form containing a file.
model_id
string
Required
The ID of the model to use for transcription, currently only ‘scribe_v1’ is available.

file
file
Required
The file to transcribe. All major audio and video formats are supported. The file size must be less than 1GB.

language_code
string
Optional
An ISO-639-1 or ISO-639-3 language_code corresponding to the language of the audio file. Can sometimes improve transcription performance if known beforehand. Defaults to null, in this case the language is predicted automatically.

tag_audio_events
boolean
Optional
Defaults to true
Whether to tag audio events like (laughter), (footsteps), etc. in the transcription.

num_speakers
integer
Optional
>=1
<=32
The maximum amount of speakers talking in the uploaded file. Can help with predicting who speaks when. The maximum amount of speakers that can be predicted is 32. Defaults to null, in this case the amount of speakers is set to the maximum value the model supports.

timestamps_granularity
enum
Optional
Defaults to word
The granularity of the timestamps in the transcription. ‘word’ provides word-level timestamps and ‘character’ provides character-level timestamps per word.

Allowed values:
none
word
character
diarize
boolean
Optional
Defaults to false
Whether to annotate which speaker is currently talking in the uploaded file.

biased_keywords
list of strings
Optional
A list of keywords and their biases. The keywords are the words that you want to bias the transcription towards. The biases decide how much the model should boost or suppress the keyword. The biases should be numbers between -10 and 10. The number of keywords cannot exceed 100. The length of each keyword must be less than 50 characters. Each keyword-bias pair must be separated by a colon. For example [“keyword_a:0.42”, “keyword_b:-0.5”]

Response
Successful Response

language_code
string
The detected language code (e.g. ‘eng’ for English).

language_probability
double
The confidence score of the language detection (0 to 1).

text
string
The raw text of the transcription.

words
list of objects
List of words with their timing information.


Hide 6 properties
text
string
The word or sound that was transcribed.

type
enum
The type of the word or sound. ‘audio_event’ is used for non-word sounds like laughter or footsteps.

Allowed values:
word
spacing
audio_event
start
double
Optional
The start time of the word or sound in seconds.

end
double
Optional
The end time of the word or sound in seconds.

speaker_id
string
Optional
Unique identifier for the speaker of this word.

characters
list of objects
Optional
The characters that make up the word and their timing information.


Show 3 properties
Errors

422
Speech to Text Convert Request Unprocessable Entity Error


ENDPOINTS
Voice Changer
Voice changer

POST


https://api.elevenlabs.io
/v1/speech-to-speech/:voice_id
POST

/v1/speech-to-speech/:voice_id

cURL

curl -X POST "https://api.elevenlabs.io/v1/speech-to-speech/JBFqnCBsd6RMkjVDRZzb?output_format=mp3_44100_128" \
     -H "xi-api-key: <apiKey>" \
     -H "Content-Type: multipart/form-data" \
     -F audio=@<file1> \
     -F model_id="eleven_multilingual_sts_v2"
Try it
Transform audio from one voice to another. Maintain full control over emotion, timing and delivery.

Path parameters
voice_id
string
Required
ID of the voice to be used. Use the Get voices endpoint list all the available voices.

Headers
xi-api-key
string
Required
Query parameters
enable_logging
boolean
Optional
Defaults to true
When enable_logging is set to false zero retention mode will be used for the request. This will mean history features are unavailable for this request, including request stitching. Zero retention mode may only be used by enterprise customers.

optimize_streaming_latency
integer
Optional
Deprecated
You can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values: 0 - default mode (no latency optimizations) 1 - normal latency optimizations (about 50% of possible latency improvement of option 3) 2 - strong latency optimizations (about 75% of possible latency improvement of option 3) 3 - max latency optimizations 4 - max latency optimizations, but also with text normalizer turned off for even more latency savings (best latency, but can mispronounce eg numbers and dates).

Defaults to None.

output_format
enum
Optional
Defaults to mp3_44100_128
Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the μ-law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs.


Search...

mp3_22050_32
mp3_44100_32
mp3_44100_64
mp3_44100_96
mp3_44100_128
mp3_44100_192
pcm_8000
pcm_16000
pcm_22050
pcm_24000
pcm_44100
ulaw_8000
Request
This endpoint expects a multipart form containing a file.
audio
file
Required
The audio file which holds the content and emotion that will control the generated speech.

model_id
string
Optional
Defaults to eleven_english_sts_v2
Identifier of the model that will be used, you can query them using GET /v1/models. The model needs to have support for speech to speech, you can check this using the can_do_voice_conversion property.

voice_settings
string
Optional
Voice settings overriding stored settings for the given voice. They are applied only on the given request. Needs to be send as a JSON encoded string.

seed
integer
Optional
If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. Must be integer between 0 and 4294967295.

remove_background_noise
boolean
Optional
Defaults to false
If set, will remove the background noise from your audio input using our audio isolation model. Only applies to Voice Changer.

Response
The generated audio file

Errors

422
Speech to Speech Convert Request Unprocessable Entity Error

Voice changer stream

POST


https://api.elevenlabs.io
/v1/speech-to-speech/:voice_id/stream
POST

/v1/speech-to-speech/:voice_id/stream

cURL

curl -X POST "https://api.elevenlabs.io/v1/speech-to-speech/JBFqnCBsd6RMkjVDRZzb/stream?output_format=mp3_44100_128" \
     -H "xi-api-key: <apiKey>" \
     -H "Content-Type: multipart/form-data" \
     -F audio=@<file1> \
     -F model_id="eleven_multilingual_sts_v2"
Try it
Stream audio from one voice to another. Maintain full control over emotion, timing and delivery.

Path parameters
voice_id
string
Required
ID of the voice to be used. Use the Get voices endpoint list all the available voices.

Headers
xi-api-key
string
Required
Query parameters
enable_logging
boolean
Optional
Defaults to true
When enable_logging is set to false zero retention mode will be used for the request. This will mean history features are unavailable for this request, including request stitching. Zero retention mode may only be used by enterprise customers.

optimize_streaming_latency
integer
Optional
Deprecated
You can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values: 0 - default mode (no latency optimizations) 1 - normal latency optimizations (about 50% of possible latency improvement of option 3) 2 - strong latency optimizations (about 75% of possible latency improvement of option 3) 3 - max latency optimizations 4 - max latency optimizations, but also with text normalizer turned off for even more latency savings (best latency, but can mispronounce eg numbers and dates).

Defaults to None.

output_format
enum
Optional
Defaults to mp3_44100_128
Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the μ-law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs.


Search...

mp3_22050_32
mp3_44100_32
mp3_44100_64
mp3_44100_96
mp3_44100_128
mp3_44100_192
pcm_8000
pcm_16000
pcm_22050
pcm_24000
pcm_44100
ulaw_8000
Request
This endpoint expects a multipart form containing a file.
audio
file
Required
The audio file which holds the content and emotion that will control the generated speech.

model_id
string
Optional
Defaults to eleven_english_sts_v2
Identifier of the model that will be used, you can query them using GET /v1/models. The model needs to have support for speech to speech, you can check this using the can_do_voice_conversion property.

voice_settings
string
Optional
Voice settings overriding stored settings for the given voice. They are applied only on the given request. Needs to be send as a JSON encoded string.

seed
integer
Optional
If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. Must be integer between 0 and 4294967295.

remove_background_noise
boolean
Optional
Defaults to false
If set, will remove the background noise from your audio input using our audio isolation model. Only applies to Voice Changer.

Response
Streaming audio data

Errors

422
Speech to Speech Convert as Stream Request Unprocessable Entity Error

Create sound effect

POST


https://api.elevenlabs.io
/v1/sound-generation
POST

/v1/sound-generation

cURL

curl -X POST https://api.elevenlabs.io/v1/sound-generation \
     -H "xi-api-key: <apiKey>" \
     -H "Content-Type: application/json" \
     -d '{
  "text": "Spacious braam suitable for high-impact movie trailer moments"
}'
Try it
Turn text into sound effects for your videos, voice-overs or video games using the most advanced sound effects model in the world.

Headers
xi-api-key
string
Required
Query parameters
output_format
enum
Optional
Defaults to mp3_44100_128
Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the μ-law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs.


Search...

mp3_22050_32
mp3_44100_32
mp3_44100_64
mp3_44100_96
mp3_44100_128
mp3_44100_192
pcm_8000
pcm_16000
pcm_22050
pcm_24000
pcm_44100
ulaw_8000
Request
This endpoint expects an object.
text
string
Required
The text that will get converted into a sound effect.

duration_seconds
double
Optional
The duration of the sound which will be generated in seconds. Must be at least 0.5 and at most 22. If set to None we will guess the optimal duration using the prompt. Defaults to None.

prompt_influence
double
Optional
A higher prompt influence makes your generation follow the prompt more closely while also making generations less variable. Must be a value between 0 and 1. Defaults to 0.3.

Response
The generated sound effect as an MP3 file

Errors

422
Text to Sound Effects Convert Request Unprocessable Entity Error

